# -*- coding: utf-8 -*-
"""Text Generation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PKtKoH-36muRjN9m5VBxAV9_Aa2Moedz
"""

!pip install transformers torch

from transformers import GPT2LMHeadModel, GPT2Tokenizer
model_name="gpt2"

# Load pre-trained model and tokenizer
model=GPT2LMHeadModel.from_pretrained(model_name)
tokenizer=GPT2Tokenizer.from_pretrained(model_name)

import torch
def generate_text(prompt, max_length=100, num_beams=5):
    # Encode the prompt text
    inputs = tokenizer.encode(prompt, return_tensors="pt")

    # Create an attention mask
    attention_mask = torch.ones(inputs.shape, dtype=torch.long)

    # Generate text using the model
    outputs = model.generate(
        inputs,
        attention_mask=attention_mask,  # Add attention mask
        max_length=max_length,
        num_beams=num_beams,
        num_return_sequences=1,
        pad_token_id=tokenizer.eos_token_id,
        no_repeat_ngram_size=2,
        early_stopping=True
    )

    # Decode the generated text
    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return generated_text

prompt = input("Enter a prompt: ")
result = generate_text(prompt)
print(result)